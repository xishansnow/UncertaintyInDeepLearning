
# 第 1 章 不确定性的重要性

在贝叶斯机器学习社区，我们使用概率模型和不确定性。高斯过程等模型定义了函数的概率分布，用于学习更有可能和不太可能的方法来从观察到的数据中进行概括。机器学习的这种概率观点为数据分析和决策提供了置信度界限，例如生物学家用来分析她的数据的信息，或者自动驾驶汽车用来决定是否刹车。在分析数据或做出决策时，通常需要能够判断模型对其输出是否确定，能够询问“也许我需要使用更多样化的数据？或改变模型？或者在做决定时要小心？”。这些问题是贝叶斯机器学习的基本问题，并且在该领域得到了广泛的研究 [Ghahramani, 2015]。另一方面，当使用深度学习模型时 [Goodfellow et al., 2016]，我们通常只有手头的参数和预测的点估计。使用此类模型迫使我们牺牲我们的工具来回答上述问题，这可能导致我们无法判断模型是在做出明智的预测还是只是随机猜测的情况

在贝叶斯机器学习界，我们在工作中使用概率模型和不确定性。高斯过程等模型定义了函数的概率分布，被用来从观察到的数据中学习归纳出更有可能和更不可能的方法。这种机器学习的概率观点为数据分析和决策提供了信念边界，例如，生物学家会依靠这些信息来分析其数据，或者一辆自动驾驶汽车会利用这些信息来决定是否刹车。在分析数据或做决策时，往往需要能够判断一个模型对其输出是否确定，能够问："也许我需要使用更多不同的数据，或者改变模型，或者在做决定时也许要小心一些"。另一方面，当使用深度学习模型时[Goodfellow等人，2016]，我们通常只有参数和预测的点估计。使用此类模型迫使我们牺牲贝叶斯工具来回答上述问题，有可能导致我们无法判断一个模型是在进行合理的预测，还是只是在随机猜测。

大多数深度学习模型通常被视为确定性函数，因此被视为在一个与拥有不确定性信息的概率模型非常不同的环境中运行。也许正是因为这个原因，看到现代深度学习与概率模型的关系如此密切是非常令人惊讶的。事实上，我们将看到，我们可以从现有的深度学习模型中免费获得不确定性信息，而无需改变任何事情。本文的主要目标是开发这样的实用工具来推断深度学习中的不确定性。

## 1.1 深度学习

为了介绍深度学习，我将从最简单的统计工具开始：线性回归[Gauss, 1809; Legendre, 1805; Seal, 1967] 。在线性回归中，我们得到一组 $N$ 个输入-输出对 $\left\{\left(\mathbf{x}_{1}, \mathbf{y}_{1}\right), \ldots,\left(\mathbf{x}_{N}, \mathbf{y}_{N}\right)\right\}$，例如二氧化碳$\mathrm{CO}_{2}$ -温度观测值，或者不同驾驶速度下的平均事故数。我们假设存在一个线性函数，将每个 $\mathbf{x}_{i} \in \mathbb{R}^{Q}$ 映射到 $\mathbf{y}_{i} \in \mathbb{R}^{D}$ （$\mathbf{y}_{i}$ 有可能被观测噪声破坏）。在这种情况下，我们的模型是输入的线性转换：$\mathbf{f}(\mathbf{x})=\mathbf{x} \mathbf{W}+\mathbf{b}$，其中 $\mathbf{W}$ 是一个 $Q$ 乘 $D$ 的矩阵，$\mathbf{b}$ 为含 $D$ 个元素的实数向量。不同的参数 $\mathbf{W}, \mathbf{b}$ 定义了不同的线性变换，我们的目的是找到参数，例如，使我们观察到的数据的平均平方误差最小：$\frac{1}{N} \sum_{i}\left\|\mathbf{y}_{i}-\left(\mathbf{x}_{i} \mathbf{W}+\mathbf{b}\right)\right\|^{2}$。

在更普遍的情况下，$\mathrm{x}$ 与 $\mathbf{y}$ 之间的关系不需要是线性的，我们可能希望定义一个非线性函数 $\mathbf{f}(\mathbf{x})$ 来映射输入和输出。为此，我们可以采用线性基函数回归[Bishop, 2006; Gergonne, 1815; Smith, 1918]，其中输入 $\mathbf{x}$ 通过 $K$ 个固定标量值的非线性变换 $\phi_{k}(\mathbf{x})$ 来组成一个特征向量 $\Phi(\mathbf{x})=\left[\phi_{1}(\mathbf{x}), \ldots, \phi_{K}(\mathbf{x})\right]$ 。然后我们用这个向量代替 $\mathbf{x}$ 本身进行线性回归。变换  $\phi_{k}$ 是我们的基础函数，对于标量输入 $x$ ，它们可以是以 $k$ 为参数的小波，不同度的多项式 $x^{k}$，或不同频率的正弦波。当 $\phi_{k}(\mathbf{x}):=x_{k}$ 和  $K=Q$ 时，基函数回归简化为线性回归。基函数通常被认为是固定的和相互正交的，并寻求这些函数的最佳组合

放宽对基函数固定和互为正交的约束，我们可以用不同的基函数代替[Bishop, 2006]。例如，我们可以将基函数定义为$\phi_{k}^{\mathbf{w}_{k}, b_{k}}$ ，其中标量值函数 $\phi_{k}$ 被应用于内积 $\left\langle\mathbf{w}_{k}, \mathbf{x}\right\rangle+b_{k}$ 。在这种情况下， $\phi_{k}$ 通常被定义为对所有的 $k$ 都是相同的，例如 $\phi_{k}(\cdot)=\sin (\cdot)$ 给出 $\phi_{k}^{\mathbf{w}_{k}, b_{k}}(\mathbf{x})=\sin \left(\left\langle\mathbf{w}_{k}, \mathbf{x}\right\rangle+b_{k}\right)$ 。由基函数输出组成的特征向量再次作为输入被送入线性变换。模型的输出可以更简洁地写成 $\mathbf{f}(\mathbf{x})=\Phi^{\mathbf{W}_{1}, \mathbf{b}_{1}}(\mathbf{x}) \mathbf{W}_{2}+\mathbf{b}_{2}$ ，其中$\Phi^{\mathbf{W}_{1}, \mathbf{b}_{1}}(\mathbf{x})=\phi\left(\mathbf{W}_{1} \mathbf{x}+\mathbf{b}_{1}\right)$, $\mathbf{W}_{1}$ 是一个维数为 $Q$ 乘 $K$ 的矩阵，$\mathbf{b}_{1}$ 是一个有 $K$ 个元素的向量， $\mathbf{W}_{2}$ 是一个维数为  $K$ 乘 $D$ 的矩阵，$\mathbf{b}_{2}$ 是一个有元素的向量。

为执行回归，现在我们可以找到  $\mathbf{W}_{1}$ ,$\mathbf{b}_{1}$  以及  $\mathbf{W}_{2}$ ,$\mathbf{b}_{2}$ ，使我们观察到的数据的平均误差最小，$\|\mathbf{y-f(x)}\|^2$ 。

